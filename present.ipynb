{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mod.logs import Logger\n",
    "from mod.dao import MyConn, SqlStatement\n",
    "from mod.clean.pca import check_series_num, prep_pca, reduce_dim\n",
    "\n",
    "from data_lake import PT_TABLE, OP_TABLE, DX_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare modules\n",
    "l = Logger()\n",
    "conn = MyConn(\"127.0.0.1\", \"hecon\", l, False)\n",
    "builder = SqlStatement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        select * from hecon.pt_dl\n",
      "        \n",
      "\n",
      "        select * from hecon.op_dl\n",
      "        \n",
      "\n",
      "        select * from hecon.dx_dl\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Load data from database\n",
    "pt = conn.wrap(builder.read_data, table=PT_TABLE)\n",
    "op = conn.wrap(builder.read_data, table=OP_TABLE)\n",
    "dx = conn.wrap(builder.read_data, table=DX_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regarding Duplication Removal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT data: Before (243, 16) / After Duplicate Removal (243, 16)\n",
      "OP data: Before (243, 43) / After Duplicate Removal (243, 43)\n",
      "DX data: Before (243, 46) / After Duplicate Removal (243, 46)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates?\n",
    "# Let's write a simple duplicate removal code. \n",
    "\n",
    "def remove_duplicates(data: np.ndarray, pivot_order=None) -> pd.DataFrame:\n",
    "    if pivot_order is None:\n",
    "        pivot_order = {\"index\": 1, \"columns\": 0, \"values\": 2}\n",
    "\n",
    "    # Create data\n",
    "    df = pd.DataFrame(data)\n",
    "    df_pivot = df.pivot(**pivot_order)\n",
    "\n",
    "    # Identify duplicate rows, where the current row is the same as the previous row\n",
    "    df_pivot[\"is_dup\"] = df_pivot.duplicated(\n",
    "        subset=df_pivot.columns.difference([\"dates\"]),\n",
    "        keep=\"first\",\n",
    "    ) & ~df_pivot.duplicated(\n",
    "        subset=df_pivot.columns.difference([\"dates\"]),\n",
    "        keep=\"last\",\n",
    "    )\n",
    "\n",
    "    # Keep only the rows that are not marked as duplicates\n",
    "    df_cleaned = df_pivot[\n",
    "        ~df_pivot[\"is_dup\"] |\n",
    "        df_pivot.duplicated(\n",
    "            subset=df_pivot.columns.difference([\"dates\"]),\n",
    "            keep=\"last\"\n",
    "        )\n",
    "        ].drop(columns=\"is_dup\")\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "# PT Data\n",
    "pt_original = pd.DataFrame(pt)\n",
    "pt_original = pt_original.pivot(index=1, columns=0, values=2).reset_index()\n",
    "pt_remove_dup = remove_duplicates(pt)\n",
    "print(\n",
    "    f\"PT data: Before {pt_original.shape} / After Duplicate Removal {pt_remove_dup.shape}\"\n",
    ")\n",
    "\n",
    "# OP Data\n",
    "op_original = pd.DataFrame(op)\n",
    "op_original = op_original.pivot(index=1, columns=0, values=2).reset_index()\n",
    "op_remove_dup = remove_duplicates(op)\n",
    "print(\n",
    "    f\"OP data: Before {op_original.shape} / After Duplicate Removal {op_remove_dup.shape}\"\n",
    ")\n",
    "\n",
    "# DX Data\n",
    "dx_original = pd.DataFrame(dx)\n",
    "dx_original = dx_original.pivot(index=1, columns=0, values=2).reset_index()\n",
    "dx_remove_dup = remove_duplicates(dx)\n",
    "print(\n",
    "    f\"DX data: Before {dx_original.shape} / After Duplicate Removal {dx_remove_dup.shape}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "PT data: Before (243, 16) / After Duplicate Removal (243, 16)\n",
    "OP data: Before (243, 43) / After Duplicate Removal (243, 43)\n",
    "DX data: Before (243, 46) / After Duplicate Removal (243, 46)\n",
    "```\n",
    "\n",
    "<h3><i>No datapoints were removed</i></h3>\n",
    "\n",
    "243개의 시계열 개수는 연구자가 주장한 243개의 총 시계열과 일치함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regarding Zero Inflation ?\n",
    "|\t| 0개수 | 전체 개수 | 0 비율 | 연구자 자료 |\n",
    "|---|------|--------|----|-----------|\n",
    "|OP | 7095 | 10206 | 0.6952 | 약 70% |\n",
    "|DX | 7467 | 10935 | 0.682853224 | 약 69% |\n",
    "\n",
    "* 90% 넘는 물품만 제거한다고 하였기 때문에, 70% 인 물품 제거 안함. \n",
    "* 이미 시점 개수가 243개인 것을 보아, 이미 정리된 자료를 준 것으로 추정됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regarding Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT time series: 15\n",
      "OP time series: 42\n",
      "DX time series: 45\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "# Reshape data by (TimeFrame * Series) Matrices using `pivot`\n",
    "ptdf = pd.DataFrame(pt)\n",
    "opdf = pd.DataFrame(op)\n",
    "dxdf = pd.DataFrame(dx)\n",
    "\n",
    "ptdf = ptdf.pivot(index=1, columns=0, values=2)\n",
    "opdf = opdf.pivot(index=1, columns=0, values=2)\n",
    "dxdf = dxdf.pivot(index=1, columns=0, values=2)\n",
    "\n",
    "# Without date column\n",
    "print(\"PT time series:\", check_series_num(ptdf))\n",
    "print(\"OP time series:\", check_series_num(opdf))\n",
    "print(\"DX time series:\", check_series_num(dxdf))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OP와 DX 데이터 프레임에서 시계열 개수는각각 42개, 45개로 파워포인트 5페이지의 \"42개 변수\", \"45개 변수\"와 일치.\n",
    "* PT를 제외하고, OP + DX 데이터 프레임을 합한 (243 * 87) 매트릭스에서 PCA를 통해 차원 축소. 목표 (87개의 시계열 -> 19개의 시계열)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP & DX data merged by dates. Shape: (215, 87)\n",
      "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0 -0.722300 -2.652508 -4.324250 -0.498422 -1.662924  2.286756  2.894187   \n",
      "1 -0.307477 -2.272311 -4.924756  0.040354 -1.644941  2.674217  3.793473   \n",
      "2 -0.307477 -2.272311 -4.924756  0.040354 -1.644941  2.674217  3.793473   \n",
      "\n",
      "        PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
      "0 -0.930957 -0.665034  2.497637  1.125673 -1.456916 -1.960308 -0.450932   \n",
      "1 -0.784862 -0.892286  1.445341  0.995391 -1.059013 -2.050220  0.381942   \n",
      "2 -0.784862 -0.892286  1.445341  0.995391 -1.059013 -2.050220  0.381942   \n",
      "\n",
      "       PC15      PC16      PC17      PC18      PC19  \n",
      "0  1.651021 -0.620240  0.412349  0.757394  1.199475  \n",
      "1  0.938856  0.029288  0.729068  0.456795  0.860014  \n",
      "2  0.938856  0.029288  0.729068  0.456795  0.860014  \n",
      "Explained Variance by each component: [0.09258922 0.07034213 0.05868283 0.05481004 0.0493887  0.04445913\n",
      " 0.04292404 0.04080997 0.03953809 0.03664917 0.03473275 0.03089689\n",
      " 0.02737723 0.02596045 0.02327613 0.02203041 0.02041967 0.0194929\n",
      " 0.01661532]\n",
      "Total Explained Variance(for the selected components): 0.7509950753342143\n"
     ]
    }
   ],
   "source": [
    "# Prep PCA Data. - Use only 215 data points to prevent data leakage\n",
    "data = prep_pca([opdf, dxdf], 215)\n",
    "print(\"OP & DX data merged by dates. Shape:\", data.shape)\n",
    "\n",
    "# Perform PCA with Standardization as default\n",
    "result =reduce_dim([opdf, dxdf], True, 215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the PCA shows that\n",
    "* The total explained variance for selected 19 components are indeed account up to 75% of the total variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add datetime information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecon23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
